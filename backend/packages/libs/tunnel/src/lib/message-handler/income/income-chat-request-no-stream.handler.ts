import { Inject, Injectable, Logger } from '@nestjs/common';
import { EventEmitter2, OnEvent } from '@nestjs/event-emitter';
import { IncomeBaseMessageHandler } from '../base-message-handler';
import {
  TunnelMessage,
  ChatRequestNoStreamMessage,
  ChatRequestNoStreamMessageSchema,
  ChatResponseMessage,
  ChatCompatibilityPayload,
  OpenAIChatCompletionRequest,
  OpenAIChatCompletionResponse,
  OpenAIChatMessage
} from '@saito/models';
import { MessageHandler } from '../message-handler.decorator';
import { TunnelService } from '../../tunnel.interface';
import {
  TUNNEL_EVENTS,
  TunnelChatRequestReceivedEvent,
  TunnelChatInferenceRequestEvent,
  TunnelInferenceResponseEvent
} from '../../events';

// ä½¿ç”¨ models ä¸­å®šä¹‰çš„ç±»å‹ï¼Œæ— éœ€æœ¬åœ°å®šä¹‰

/**
 * OpenAI éæµå¼èŠå¤©è¯·æ±‚å¤„ç†å™¨
 * 
 * èŒè´£ï¼š
 * 1. æ¥æ”¶å¹¶éªŒè¯éæµå¼èŠå¤©è¯·æ±‚
 * 2. è°ƒç”¨æ¨ç†æœåŠ¡æ‰§è¡ŒèŠå¤©
 * 3. å¤„ç†å®Œæ•´çš„ OpenAI å“åº”
 * 4. è½¬å‘å“åº”ç»™ç›®æ ‡è®¾å¤‡
 * 
 * è®¾è®¡æ¨¡å¼ï¼š
 * - Strategy Pattern: å¤„ç†ä¸åŒæ ¼å¼çš„è¯·æ±‚æ•°æ®
 * - Template Method: æ ‡å‡†åŒ–æ¶ˆæ¯å¤„ç†æµç¨‹
 * - Factory Pattern: åˆ›å»ºå“åº”å¤„ç†å™¨
 */
@MessageHandler({ type: 'chat_request_no_stream', direction: 'income' })
@Injectable()
export class IncomeChatRequestNoStreamHandler extends IncomeBaseMessageHandler {
  private readonly logger = new Logger(IncomeChatRequestNoStreamHandler.name);

  constructor(
    @Inject('TunnelService') private readonly tunnel: TunnelService,
    private readonly eventEmitter: EventEmitter2
  ) {
    super();
  }

  /**
   * å¤„ç†å…¥ç«™éæµå¼èŠå¤©è¯·æ±‚æ¶ˆæ¯
   * Template Method Pattern - å®šä¹‰æ ‡å‡†å¤„ç†æµç¨‹
   */
  async handleIncomeMessage(message: TunnelMessage): Promise<void> {
    this.logger.log(`ğŸ¯ æ”¶åˆ°éæµå¼èŠå¤©è¯·æ±‚ - ç›®æ ‡: ${message.to}, æ¥æº: ${message.from}`);

    try {
      // 1. éªŒè¯å¹¶è§£ææ¶ˆæ¯
      const chatRequest = this.parseAndValidateMessage(message);

      // å‘å°„èŠå¤©è¯·æ±‚æ¥æ”¶äº‹ä»¶
      this.eventEmitter.emit(
        TUNNEL_EVENTS.CHAT_REQUEST_RECEIVED,
        new TunnelChatRequestReceivedEvent(
          chatRequest.payload.taskId || `${Date.now()}`,
          chatRequest.from,
          chatRequest.payload,
          false // éæµå¼è¯·æ±‚
        )
      );

      // 2. æ‰§è¡Œéæµå¼èŠå¤©æ¨ç†
      await this.processChatRequestNoStream(chatRequest);

    } catch (error) {
      this.logger.error(`âŒ å¤„ç†éæµå¼èŠå¤©è¯·æ±‚å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`);
      await this.handleError(message, error);
    }
  }

  /**
   * è§£æå¹¶éªŒè¯æ¶ˆæ¯
   * Strategy Pattern - æ”¯æŒå¤šç§æ¶ˆæ¯æ ¼å¼
   */
  private parseAndValidateMessage(message: TunnelMessage): ChatRequestNoStreamMessage {
    // å°è¯•æ ‡å‡†æ ¼å¼è§£æ
    const parseResult = ChatRequestNoStreamMessageSchema.safeParse(message);

    if (parseResult.success) {
      this.logger.debug(`âœ… æ ‡å‡†æ ¼å¼è§£ææˆåŠŸ`);
      return parseResult.data;
    }

    // å°è¯•å…¼å®¹æ ¼å¼è§£æ
    this.logger.debug(`âš ï¸ æ ‡å‡†æ ¼å¼è§£æå¤±è´¥ï¼Œå°è¯•å…¼å®¹æ ¼å¼: ${parseResult.error.message}`);
    return this.parseCompatibilityFormat(message);
  }

  /**
   * è§£æå…¼å®¹æ ¼å¼çš„æ¶ˆæ¯ï¼ˆåµŒå¥— data æ ¼å¼ï¼‰
   */
  private parseCompatibilityFormat(message: TunnelMessage): ChatRequestNoStreamMessage {
    const payload = message.payload as ChatCompatibilityPayload;

    if (!payload.taskId || !payload.data) {
      throw new Error('Invalid compatibility format: missing taskId or data');
    }

    // è½¬æ¢æ¶ˆæ¯æ ¼å¼ä»¥ç¬¦åˆ OpenAI æ ‡å‡†
    const convertedMessages: OpenAIChatMessage[] = payload.data.messages.map(msg => ({
      role: msg.role as 'system' | 'user' | 'assistant' | 'function',
      content: msg.content || null,
      name: undefined,
      function_call: undefined
    }));

    const convertedData: OpenAIChatCompletionRequest = {
      model: payload.data.model || 'unknown',
      messages: convertedMessages,
      stream: false, // ç¡®ä¿æ˜¯éæµå¼è¯·æ±‚
      temperature: payload.data.temperature,
      max_tokens: payload.data.max_tokens,
      top_p: payload.data.top_p,
      frequency_penalty: payload.data.frequency_penalty,
      presence_penalty: payload.data.presence_penalty,
      stop: payload.data.stop,
      n: payload.data.n || 1,
      logit_bias: payload.data.logit_bias,
      user: payload.data.user
    };

    // è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
    return {
      type: 'chat_request_no_stream',
      from: message.from,
      to: message.to,
      payload: {
        taskId: payload.taskId,
        path: payload.path || '/openai/v1/chat/completions',
        data: {
          messages: convertedMessages.map(msg => ({
            role: msg.role as 'system' | 'user' | 'assistant',
            content: msg.content || ''
          })),
          model: convertedData.model,
          temperature: convertedData.temperature,
          max_tokens: convertedData.max_tokens,
          top_p: convertedData.top_p,
          frequency_penalty: convertedData.frequency_penalty,
          presence_penalty: convertedData.presence_penalty
        }
      }
    };
  }

  /**
   * å¤„ç†é”™è¯¯æƒ…å†µ
   */
  private async handleError(message: TunnelMessage, error: unknown): Promise<void> {
    const errorMessage = error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯';

    // å‘é€é”™è¯¯å“åº”ç»™å®¢æˆ·ç«¯
    await this.sendErrorResponse(message, errorMessage);

    this.logger.error(`å¤„ç†æ¶ˆæ¯å¤±è´¥: ${errorMessage}`, {
      messageType: message.type,
      from: message.from,
      to: message.to
    });
  }

  /**
   * å¤„ç†éæµå¼èŠå¤©è¯·æ±‚å¹¶æ‰§è¡Œæ¨ç†
   */
  private async processChatRequestNoStream(message: ChatRequestNoStreamMessage): Promise<void> {
    const payload = message.payload;
    const taskId = payload.taskId;
    const path = payload.path;

    // æ„å»º OpenAI æ ¼å¼çš„è¯·æ±‚å‚æ•°
    const requestParams: OpenAIChatCompletionRequest = {
      model: payload.data.model || 'unknown',
      messages: payload.data.messages.map(msg => ({
        role: msg.role as 'system' | 'user' | 'assistant' | 'function',
        content: msg.content,
        name: undefined,
        function_call: undefined
      })),
      stream: false, // ç¡®ä¿æ˜¯éæµå¼è¯·æ±‚
      temperature: payload.data.temperature,
      max_tokens: payload.data.max_tokens,
      top_p: payload.data.top_p,
      frequency_penalty: payload.data.frequency_penalty,
      presence_penalty: payload.data.presence_penalty,
      n: 1
    };

    this.logger.log(`æ‰§è¡Œéæµå¼èŠå¤©æ¨ç† - TaskID: ${taskId}, Path: ${path}, Model: ${requestParams.model}`);

    // éªŒè¯è¯·æ±‚æ•°æ®
    this.validateChatRequest(requestParams);

    this.logger.debug(`è°ƒç”¨æ¨ç†æœåŠ¡ - Model: ${requestParams.model}, Messages: ${requestParams.messages.length}`);

    try {
      // å‘å°„èŠå¤©æ¨ç†è¯·æ±‚äº‹ä»¶ï¼Œè®©æ¨ç†æœåŠ¡æ¨¡å—å¤„ç†
      this.eventEmitter.emit(
        TUNNEL_EVENTS.CHAT_INFERENCE_REQUEST,
        new TunnelChatInferenceRequestEvent(
          taskId,
          message.from,
          requestParams,
          path,
          false // éæµå¼è¯·æ±‚
        )
      );

      this.logger.log(`âœ… å·²å‘å°„èŠå¤©æ¨ç†è¯·æ±‚äº‹ä»¶ - TaskID: ${taskId}`);

    } catch (error) {
      this.logger.error(`å‘å°„æ¨ç†è¯·æ±‚äº‹ä»¶å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`);
      throw error;
    }
  }

  /**
   * éªŒè¯èŠå¤©è¯·æ±‚æ•°æ®
   */
  private validateChatRequest(data: OpenAIChatCompletionRequest): void {
    if (!data.messages || data.messages.length === 0) {
      throw new Error('Invalid chat request: missing messages');
    }

    if (!data.model) {
      throw new Error('Invalid chat request: missing model');
    }
  }

  // æ³¨æ„ï¼šæ¨ç†å“åº”ç›‘å¬å™¨å·²ç§»é™¤
  // ç°åœ¨æ¨ç†æœåŠ¡ç›´æ¥é€šè¿‡ tunnel å‘é€å“åº”ï¼Œä¸å†éœ€è¦äº‹ä»¶è½¬å‘

  /**
   * åˆ›å»ºéæµå¼å“åº”å¤„ç†å™¨
   * Factory Pattern - åˆ›å»ºæ¨¡æ‹Ÿçš„ Express Response å¯¹è±¡
   */
  private createNoStreamResponseHandler(taskId: string, targetDeviceId: string) {
    return {
      // Express Response æ¥å£æ–¹æ³•
      setHeader: () => { },
      status: () => ({ json: () => { } }),
      headersSent: false,

      // éæµå¼å“åº”æ–¹æ³•
      json: async (data: OpenAIChatCompletionResponse) => {
        try {
          this.logger.debug(`ğŸ“¤ æ”¶åˆ°å®Œæ•´å“åº” - TaskID: ${taskId}, Model: ${data.model}`);
          await this.sendCompleteResponse(taskId, targetDeviceId, data);
        } catch (error) {
          this.logger.error(`å¤„ç†å®Œæ•´å“åº”å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`);
        }
      },

      // å…¼å®¹æµå¼æ¥å£ï¼ˆä½†ä¸ä¼šè¢«è°ƒç”¨ï¼‰
      write: () => { },
      end: () => { }
    };
  }

  /**
   * å‘é€å®Œæ•´å“åº”
   */
  private async sendCompleteResponse(taskId: string, targetDeviceId: string, response: OpenAIChatCompletionResponse): Promise<void> {
    this.logger.log(`âœ… éæµå¼æ¨ç†å®Œæˆ - TaskID: ${taskId}, Target: ${targetDeviceId}`);

    const responseMessage: ChatResponseMessage = {
      type: 'chat_response',
      from: this.peerId,
      to: targetDeviceId,
      payload: {
        taskId,
        data: response
      }
    };

    // ä½¿ç”¨ handleMessage è®©ç³»ç»Ÿè‡ªåŠ¨åˆ¤æ–­å‘é€ç›®æ ‡
    await this.tunnel.handleMessage(responseMessage as any);
  }

  /**
   * å‘é€é”™è¯¯å“åº”
   */
  private async sendErrorResponse(originalMessage: TunnelMessage, error: string): Promise<void> {
    const payload = originalMessage.payload as { taskId?: string };
    const taskId = payload.taskId || `error-${Date.now()}`;

    const errorResponse: OpenAIChatCompletionResponse = {
      id: `chatcmpl-error-${taskId}`,
      object: 'chat.completion',
      created: Math.floor(Date.now() / 1000),
      model: 'unknown',
      choices: [{
        index: 0,
        message: {
          role: 'assistant',
          content: `å‘ç”Ÿé”™è¯¯: ${error}`,
          name: undefined,
          function_call: undefined
        },
        finish_reason: 'stop'
      }],
      usage: {
        prompt_tokens: 0,
        completion_tokens: 0,
        total_tokens: 0
      }
    };

    const errorMessage: ChatResponseMessage = {
      type: 'chat_response',
      from: this.peerId,
      to: originalMessage.from,
      payload: {
        taskId,
        data: errorResponse,
        error
      }
    };

    await this.tunnel.handleMessage(errorMessage as any);
  }
}
